#importing libraries
import pandas as pd
import numpy as np
from scipy import stats
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Import data
filepath = "C://Users//ADMIN//OneDrive//Desktop//HistoricalPrices data.csv"
eabldata = pd.read_csv(filepath)

## Inspecting the data
eabldata.info()
eabldata.describe()
eabldata.head()
eabldata.tail()

# Handling missing values
eabldata.isnull().sum()

# Remove duplicates if any
eabldata.drop_duplicates(inplace=True)

# Remove leading spaces from column names
eabldata.columns = eabldata.columns.str.strip()

# Fix Data Types
eabldata["Date"] = pd.to_datetime(eabldata["Date"], format="%m/%d/%Y", errors="coerce")

# Handling outliers using IQR method
Q1 = eabldata["Low"].quantile(0.25)
Q3 = eabldata["Low"].quantile(0.75)
IQR = Q3 - Q1

# Filtering rows using IQR
eabldata = eabldata[(eabldata["Low"] >= Q1 - 1.5 * IQR) & (eabldata["Low"] <= Q3 + 1.5 * IQR)]

# Use Z-score to detect and remove extreme outliers (beyond 3 standard deviations)
eabldata = eabldata[np.abs(stats.zscore(eabldata["Low"])) < 3]

# Normalize and standardize data
scaler_minmax = MinMaxScaler()  # Min-Max Scaling (0-1)
scaler_standard = StandardScaler()  # Standardization (mean=0, std=1)

# Apply transformations correctly
eabldata["Low"] = scaler_minmax.fit_transform(eabldata[["Low"]])
eabldata["Low"] = scaler_standard.fit_transform(eabldata[["Low"]])

# Save the cleaned data
eabldata.to_csv('cleaned_data.csv', index=False)

print("Data preprocessing complete. Cleaned data saved as 'cleaned_data.csv'.")
